{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab54e3d7",
   "metadata": {},
   "source": [
    "# Time-Ordered Data Preparation for Multi-Gateway RSSI Fingerprinting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cad2a0",
   "metadata": {},
   "source": [
    "### Goals\n",
    "- Load the cleaned multi-gateway dataset.\n",
    "- Enforce strict time ordering.\n",
    "- Leakage-safe 80/20 split by physical uplink (all gateway receptions on one side).\n",
    "- 5 time-series folds within the training slice (no shuffling).\n",
    "- Save time-ordered train/test splits and fold assignments for downstream models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "295654bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "\n",
    "DATA_PATH = \"../LoRaWAN Localization - Advanced Files/all3_gateways_kalman_filtered.csv\"\n",
    "SAVE_DIR = \"../LoRaWAN Localization - Advanced Files\"\n",
    "TEST_FRAC = 0.20   # hold out most recent 20%\n",
    "N_FOLDS = 5        # folds within training slice\n",
    "GAP = 0            # set >0 to enforce a temporal gap between train/val folds\n",
    "\n",
    "gateway_col = \"gateway\"\n",
    "device_col = \"device_id\"\n",
    "f_cnt_col = \"f_count\"\n",
    "\n",
    "target_col = \"exp_pl_gw0\"  # adjust if your target differs\n",
    "\n",
    "feature_names = [\n",
    "    \"co2\", \"humidity\", \"pm25\", \"pressure\", \"temperature\",\n",
    "    \"rssi\", \"snr\", \"SF\", \"frequency\", \"f_count\", \"p_count\", \"toa\",\n",
    "    \"distance_gw0\", \"c_walls_gw0\", \"w_walls_gw0\", \"n_power\", \"esp\",\n",
    "]\n",
    "context_cols = [\"time\", gateway_col, device_col, \"uplink_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a61e4",
   "metadata": {},
   "source": [
    "### Load, Normalize Time, and Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f4c2cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after load/sort: (3411800, 25)\n",
      "Time span: 2024-10-01 00:01:07.420593+00:00 -> 2025-09-30 23:59:55.971870+00:00\n",
      "Columns: ['time', 'gateway', 'device_id', 'co2', 'humidity', 'pm25', 'pressure', 'temperature', 'rssi', 'snr', 'SF', 'frequency', 'f_count', 'p_count', 'toa', 'distance_gw0', 'c_walls_gw0', 'w_walls_gw0', 'exp_pl_gw0', 'n_power', 'esp', 'filtered_rssi', 'kf_rssi', 'exp_pl_filtered_gw0', 'exp_pl_kf_gw0']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "\n",
    "if \"time\" not in df.columns:\n",
    "    raise KeyError(\"Column 'time' not found.\")\n",
    "\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\", utc=True)\n",
    "df = df.dropna(subset=[\"time\"]).sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "print(\"Data shape after load/sort:\", df.shape)\n",
    "print(\"Time span:\", df[\"time\"].min(), \"->\", df[\"time\"].max())\n",
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4f55b4",
   "metadata": {},
   "source": [
    "### Uplink Identity (prevents cross-gateway leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "688817b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uplink_id constructed from device + f_count + floored time.\n"
     ]
    }
   ],
   "source": [
    "df = df.copy()\n",
    "if \"uplink_id\" not in df.columns:\n",
    "    if {device_col, f_cnt_col}.issubset(df.columns):\n",
    "        df[\"uplink_id\"] = (\n",
    "            df[device_col].fillna(\"na_device\").astype(str)\n",
    "            + \"_\"\n",
    "            + df[f_cnt_col].fillna(-1).astype(int).astype(str)\n",
    "            + \"_\"\n",
    "            + df[\"time\"].dt.floor(\"1s\").astype(str)\n",
    "        )\n",
    "        print(\"uplink_id constructed from device + f_count + floored time.\")\n",
    "    else:\n",
    "        df[\"uplink_id\"] = pd.NA\n",
    "        print(\"uplink_id not built (missing device/f_count); splits will use row order only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b451036",
   "metadata": {},
   "source": [
    "### Leakage-Safe 80/20 Time Split by Uplink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31d41aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2714781, Test samples: 697019\n",
      "Train window: 2024-10-01 00:01:07.420593+00:00 -> 2025-08-12 14:10:35.483231+00:00\n",
      "Test  window: 2025-08-12 14:10:39.834945+00:00 -> 2025-09-30 23:59:55.971870+00:00\n"
     ]
    }
   ],
   "source": [
    "if df[\"uplink_id\"].isna().all():\n",
    "    split_idx = int(len(df) * (1 - TEST_FRAC))\n",
    "    train_df = df.iloc[:split_idx].copy()\n",
    "    test_df  = df.iloc[split_idx:].copy()\n",
    "else:\n",
    "    uplink_first_time = df.groupby(\"uplink_id\", dropna=False)[\"time\"].min().sort_values()\n",
    "    split_uplink_idx = int(len(uplink_first_time) * (1 - TEST_FRAC))\n",
    "    train_uplinks = set(uplink_first_time.index[:split_uplink_idx])\n",
    "    test_uplinks  = set(uplink_first_time.index[split_uplink_idx:])\n",
    "    train_df = df[df[\"uplink_id\"].isin(train_uplinks)].copy()\n",
    "    test_df  = df[df[\"uplink_id\"].isin(test_uplinks)].copy()\n",
    "\n",
    "train_df = train_df.sort_values(\"time\").reset_index(drop=True)\n",
    "test_df  = test_df.sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Test samples: {len(test_df)}\")\n",
    "print(f\"Train window: {train_df.time.min()} -> {train_df.time.max()}\")\n",
    "print(f\"Test  window: {test_df.time.min()} -> {test_df.time.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c51efa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap uplink_ids: 0\n",
      "Unique uplinks total: 2111204\n",
      "Train unique uplinks: 1688963\n",
      "Test unique uplinks : 422241\n",
      "Train + Test equals total: True\n"
     ]
    }
   ],
   "source": [
    "# Verify uplink_id exclusivity across train/test\n",
    "train_uids = set(train_df[\"uplink_id\"].dropna().unique())\n",
    "test_uids  = set(test_df[\"uplink_id\"].dropna().unique())\n",
    "overlap = train_uids & test_uids\n",
    "print(\"Overlap uplink_ids:\", len(overlap))  # expect 0\n",
    "\n",
    "print(\"Unique uplinks total:\", df[\"uplink_id\"].nunique())\n",
    "print(\"Train unique uplinks:\", train_df[\"uplink_id\"].nunique())\n",
    "print(\"Test unique uplinks :\", test_df[\"uplink_id\"].nunique())\n",
    "print(\n",
    "    \"Train + Test equals total:\",\n",
    "    train_df[\"uplink_id\"].nunique() + test_df[\"uplink_id\"].nunique() == df[\"uplink_id\"].nunique(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f593409",
   "metadata": {},
   "source": [
    "### Feature/Target Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85317575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (2714781, 17) | X_test: (697019, 17)\n",
      "y_train: (2714781,) | y_test: (697019,)\n"
     ]
    }
   ],
   "source": [
    "missing_feats = [f for f in feature_names if f not in df.columns]\n",
    "if missing_feats:\n",
    "    raise KeyError(f\"Missing features in data: {missing_feats}\")\n",
    "if target_col not in df.columns:\n",
    "    raise KeyError(f\"Target column '{target_col}' not found.\")\n",
    "\n",
    "X_train = train_df[feature_names].to_numpy()\n",
    "y_train = train_df[target_col].to_numpy()\n",
    "time_train = train_df[\"time\"].to_numpy()\n",
    "\n",
    "X_test = test_df[feature_names].to_numpy()\n",
    "y_test = test_df[target_col].to_numpy()\n",
    "time_test = test_df[\"time\"].to_numpy()\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"| X_test:\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape, \"| y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4e481",
   "metadata": {},
   "source": [
    "### Save Time-Ordered Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d546635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train.csv and test.csv to ../LoRaWAN Localization - Advanced Files\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "def assemble_out(df_part):\n",
    "    cols_out = list(feature_names) + [target_col, \"time\"]\n",
    "    for c in context_cols:\n",
    "        if c not in cols_out and c in df_part.columns:\n",
    "            cols_out.append(c)\n",
    "    return df_part[cols_out].copy()\n",
    "\n",
    "train_out = assemble_out(train_df)\n",
    "test_out  = assemble_out(test_df)\n",
    "\n",
    "train_out.to_csv(f\"{SAVE_DIR}/train.csv\", index=False)\n",
    "test_out.to_csv(f\"{SAVE_DIR}/test.csv\", index=False)\n",
    "print(f\"Saved train.csv and test.csv to {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1f01a",
   "metadata": {},
   "source": [
    "### Time-Series Cross-Validation Folds (Training Slice Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cb537ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: val window 2024-11-22 21:14:17.215715+00:00 -> 2025-01-12 19:32:13.537631+00:00\n",
      "Fold 2: val window 2025-01-12 19:32:48.589100+00:00 -> 2025-03-16 00:02:36.281205+00:00\n",
      "Fold 3: val window 2025-03-16 00:02:42.676390+00:00 -> 2025-05-06 12:10:28.102176+00:00\n",
      "Fold 4: val window 2025-05-06 12:10:46.114562+00:00 -> 2025-06-29 23:09:03.071242+00:00\n",
      "Fold 5: val window 2025-06-29 23:09:15.793512+00:00 -> 2025-08-12 14:10:35.483231+00:00\n",
      "Saved time-ordered 5-fold assignments to ../LoRaWAN Localization - Advanced Files/train_folds.npy\n"
     ]
    }
   ],
   "source": [
    "# Time-series folds grouped by uplink_id (training slice only)\n",
    "tscv = TimeSeriesSplit(n_splits=N_FOLDS, gap=GAP)\n",
    "fold_assignments = np.zeros(len(train_df), dtype=int)\n",
    "\n",
    "if train_df[\"uplink_id\"].isna().all():\n",
    "    # Fallback: row-wise folds (still time-ordered)\n",
    "    for fold_num, (_, val_idx) in enumerate(tscv.split(train_df), start=1):\n",
    "        fold_assignments[val_idx] = fold_num\n",
    "        val_start, val_end = time_train[val_idx].min(), time_train[val_idx].max()\n",
    "        print(f\"Fold {fold_num}: val window {val_start} -> {val_end}\")\n",
    "else:\n",
    "    uplink_first = train_df.groupby(\"uplink_id\", dropna=False)[\"time\"].min().sort_values()\n",
    "    uplinks_ordered = uplink_first.index.to_numpy()\n",
    "\n",
    "    for fold_num, (tr_u_idx, val_u_idx) in enumerate(tscv.split(uplinks_ordered), start=1):\n",
    "        tr_u = set(uplinks_ordered[tr_u_idx])\n",
    "        val_u = set(uplinks_ordered[val_u_idx])\n",
    "\n",
    "        val_mask = train_df[\"uplink_id\"].isin(val_u)\n",
    "        fold_assignments[val_mask.to_numpy().nonzero()[0]] = fold_num\n",
    "\n",
    "        val_times = train_df.loc[val_mask, \"time\"]\n",
    "        print(f\"Fold {fold_num}: val window {val_times.min()} -> {val_times.max()}\")\n",
    "\n",
    "np.save(f\"{SAVE_DIR}/train_folds.npy\", fold_assignments)\n",
    "print(f\"Saved time-ordered {N_FOLDS}-fold assignments to {SAVE_DIR}/train_folds.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa20db1",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Encode `gateway` before modeling (one-hot/embedding/target encoding); raw string is kept for context.\n",
    "- If you expect gateway RSSI bias, calibrate per gateway before training.\n",
    "- Increase `GAP` if you want a temporal buffer between train/val folds.\n",
    "- For strict leakage control, keep the uplink-based split and apply the same grouping when batching for models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
